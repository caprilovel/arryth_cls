{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "679c1d8f-b67b-4231-8c77-08d77fadf843",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8da9b760-6efb-4a22-b263-a4cbf8869f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_channels, out_channels, stride=1,\n",
    "           padding=1, bias=True, groups=1):\n",
    "    return nn.Conv2d(in_channels, out_channels,\n",
    "                     kernel_size=3, stride=stride,\n",
    "                     padding=padding, bias=bias,\n",
    "                     groups=groups\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d6a50c5-9831-4744-84aa-041f96684c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1x1(in_channels, out_channels, groups=1):\n",
    "    return nn.Conv2d(\n",
    "        in_channels, \n",
    "        out_channels,\n",
    "        kernel_size=1,\n",
    "        groups =groups,\n",
    "        stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df29aca4-da11-47f0-a5dc-c62796356482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_shuffle(x, groups):\n",
    "    batchsize, num_channels, height, width = x.data.size()\n",
    "    \n",
    "    channels_per_group = num_channels // groups\n",
    "    \n",
    "    x = x.view(batchsize, groups,\n",
    "               channels_per_group, height, width)\n",
    "    \n",
    "    x = torch.transpose(x, 1, 2).contiguous()\n",
    "    \n",
    "    x = x.view(batchsize, -1, height, width)\n",
    "    \n",
    "    return x   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0067fce7-93fc-45a4-8286-8f0f6293a43e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ShuffleUnit(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,\n",
    "                 groups=3, grouped_conv=True, \n",
    "                 combine='add'):\n",
    "        super(ShuffleUnit, self).__init__()\n",
    "        self.in_channnels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.grouped_conv = grouped_conv \n",
    "        self.bottleneck_channels = self.out_channels // 4\n",
    "        \n",
    "        if self.combine == 'add':\n",
    "            self.depthwise_stride = 1 \n",
    "            self._combine_func = self._add\n",
    "            \n",
    "        elif self.combine == 'concat':\n",
    "            self.depthwise_stride = 2 \n",
    "            self._combine_func = self._concat\n",
    "            \n",
    "            self.out_channels -= slef.in_channels \n",
    "        \n",
    "        else: \n",
    "            raise ValueError(\"Cannot combine tensors with \\\"{}\\\"\" \\\n",
    "                             \"Only \\\"add\\\" and \\\"concat\\\" are\" \\\n",
    "                             \"supported\".format(self.combine))\n",
    "        \n",
    "        self.first_1x1_groups = self.groups if grouped_conv else 1\n",
    "        self.first_1x1_compress = self._make_grouped_conv1x1(\n",
    "            self.in_channels,\n",
    "            self.bottleneck_channels,\n",
    "            self.first_1x1_groups,\n",
    "            batch_norm=True,\n",
    "            relu=True\n",
    "            )\n",
    "        \n",
    "    @staticmethod\n",
    "    def _add(x, out):\n",
    "        return x + out\n",
    "    \n",
    "    \n",
    "    @staticmethod \n",
    "    def _concat(x, out):\n",
    "        return torch.cat((x, out), 1)\n",
    "    \n",
    "    def _make_grouped_conv1x1(self, in_channels, out_channels, groups,\n",
    "                              batch_norm=True, relu=False):\n",
    "        modules = OrderedDict()\n",
    "        \n",
    "        conv = conv1x1(in_channels, out_channels, groups=groups)\n",
    "        modules['conv1x1'] = conv \n",
    "        \n",
    "        if batch_norm:\n",
    "            modules['batch_norm'] = nn.BatchNorm2d(out_channels)\n",
    "        if relu:\n",
    "            modules['relu'] = nn.ReLU()\n",
    "        if len(modules) > 1: \n",
    "            return nn.Sequential(modules)\n",
    "        else: \n",
    "            return conv\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "            \n",
    "        if self.combine == 'concat':\n",
    "            residual = F.avg_pool2d(residual ,kernel_szie=3,\n",
    "                                    stride=2, padding=1)\n",
    "            \n",
    "        out = self.g_conv_1x1_compress(x)\n",
    "        out = channel_shuffle(out, self.groups)\n",
    "        out = self.depthwise_conv3x3(out)\n",
    "        out = self.bn_after_depthwise(out)\n",
    "        out = self.g_conv_1x1_expand(out)\n",
    "        \n",
    "        out = self._combine_func(residual, out)\n",
    "        return F.relu(out)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ccb7fa-bc9c-4181-9de4-55dd4e37915e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShuffleNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, groups=3, in_channels=3, num_classes=1000):\n",
    "        \"\"\"ShuffleNet constructor.\n",
    "        Arguments:\n",
    "            groups (int, optional): number of groups to be used in grouped \n",
    "                1x1 convolutions in each ShuffleUnit. Default is 3 for best\n",
    "                performance according to original paper.\n",
    "            in_channels (int, optional): number of channels in the input tensor.\n",
    "                Default is 3 for RGB image inputs.\n",
    "            num_classes (int, optional): number of classes to predict. Default\n",
    "                is 1000 for ImageNet.\n",
    "        \"\"\"\n",
    "        super(ShuffleNet, self).__init__()\n",
    "\n",
    "        self.groups = groups\n",
    "        self.stage_repeats = [3, 7, 3]\n",
    "        self.in_channels =  in_channels\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # index 0 is invalid and should never be called.\n",
    "        # only used for indexing convenience.\n",
    "        if groups == 1:\n",
    "            self.stage_out_channels = [-1, 24, 144, 288, 567]\n",
    "        elif groups == 2:\n",
    "            self.stage_out_channels = [-1, 24, 200, 400, 800]\n",
    "        elif groups == 3:\n",
    "            self.stage_out_channels = [-1, 24, 240, 480, 960]\n",
    "        elif groups == 4:\n",
    "            self.stage_out_channels = [-1, 24, 272, 544, 1088]\n",
    "        elif groups == 8:\n",
    "            self.stage_out_channels = [-1, 24, 384, 768, 1536]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"\"\"{} groups is not supported for\n",
    "                   1x1 Grouped Convolutions\"\"\".format(num_groups))\n",
    "        \n",
    "        # Stage 1 always has 24 output channels\n",
    "        self.conv1 = conv3x3(self.in_channels,\n",
    "                             self.stage_out_channels[1], # stage 1\n",
    "                             stride=2)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Stage 2\n",
    "        self.stage2 = self._make_stage(2)\n",
    "        # Stage 3\n",
    "        self.stage3 = self._make_stage(3)\n",
    "        # Stage 4\n",
    "        self.stage4 = self._make_stage(4)\n",
    "\n",
    "        # Global pooling:\n",
    "        # Undefined as PyTorch's functional API can be used for on-the-fly\n",
    "        # shape inference if input size is not ImageNet's 224x224\n",
    "\n",
    "        # Fully-connected classification layer\n",
    "        num_inputs = self.stage_out_channels[-1]\n",
    "        self.fc = nn.Linear(num_inputs, self.num_classes)\n",
    "        self.init_params()\n",
    "\n",
    "\n",
    "    def init_params(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    init.constant(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant(m.weight, 1)\n",
    "                init.constant(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal(m.weight, std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    init.constant(m.bias, 0)\n",
    "\n",
    "\n",
    "    def _make_stage(self, stage):\n",
    "        modules = OrderedDict()\n",
    "        stage_name = \"ShuffleUnit_Stage{}\".format(stage)\n",
    "        \n",
    "        # First ShuffleUnit in the stage\n",
    "        # 1. non-grouped 1x1 convolution (i.e. pointwise convolution)\n",
    "        #   is used in Stage 2. Group convolutions used everywhere else.\n",
    "        grouped_conv = stage > 2\n",
    "        \n",
    "        # 2. concatenation unit is always used.\n",
    "        first_module = ShuffleUnit(\n",
    "            self.stage_out_channels[stage-1],\n",
    "            self.stage_out_channels[stage],\n",
    "            groups=self.groups,\n",
    "            grouped_conv=grouped_conv,\n",
    "            combine='concat'\n",
    "            )\n",
    "        modules[stage_name+\"_0\"] = first_module\n",
    "\n",
    "        # add more ShuffleUnits depending on pre-defined number of repeats\n",
    "        for i in range(self.stage_repeats[stage-2]):\n",
    "            name = stage_name + \"_{}\".format(i+1)\n",
    "            module = ShuffleUnit(\n",
    "                self.stage_out_channels[stage],\n",
    "                self.stage_out_channels[stage],\n",
    "                groups=self.groups,\n",
    "                grouped_conv=True,\n",
    "                combine='add'\n",
    "                )\n",
    "            modules[name] = module\n",
    "\n",
    "        return nn.Sequential(modules)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "\n",
    "        # global average pooling layer\n",
    "        x = F.avg_pool2d(x, x.data.size()[-2:])\n",
    "        \n",
    "        # flatten for input to fully-connected layer\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765bd0f5-d0a1-4cda-82ce-d5c4e22ac0a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "daee81ca-b55d-472e-9531-23b5b591cb8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[  0,   1,   2,   3],\n",
       "          [  4,   5,   6,   7],\n",
       "          [  8,   9,  10,  11],\n",
       "          [ 12,  13,  14,  15]],\n",
       "\n",
       "         [[ 64,  65,  66,  67],\n",
       "          [ 68,  69,  70,  71],\n",
       "          [ 72,  73,  74,  75],\n",
       "          [ 76,  77,  78,  79]],\n",
       "\n",
       "         [[ 16,  17,  18,  19],\n",
       "          [ 20,  21,  22,  23],\n",
       "          [ 24,  25,  26,  27],\n",
       "          [ 28,  29,  30,  31]],\n",
       "\n",
       "         [[ 80,  81,  82,  83],\n",
       "          [ 84,  85,  86,  87],\n",
       "          [ 88,  89,  90,  91],\n",
       "          [ 92,  93,  94,  95]],\n",
       "\n",
       "         [[ 32,  33,  34,  35],\n",
       "          [ 36,  37,  38,  39],\n",
       "          [ 40,  41,  42,  43],\n",
       "          [ 44,  45,  46,  47]],\n",
       "\n",
       "         [[ 96,  97,  98,  99],\n",
       "          [100, 101, 102, 103],\n",
       "          [104, 105, 106, 107],\n",
       "          [108, 109, 110, 111]],\n",
       "\n",
       "         [[ 48,  49,  50,  51],\n",
       "          [ 52,  53,  54,  55],\n",
       "          [ 56,  57,  58,  59],\n",
       "          [ 60,  61,  62,  63]],\n",
       "\n",
       "         [[112, 113, 114, 115],\n",
       "          [116, 117, 118, 119],\n",
       "          [120, 121, 122, 123],\n",
       "          [124, 125, 126, 127]]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensora = torch.arange(0,128)\n",
    "tensora = tensora.reshape(1, 8, 4, -1)\n",
    "\n",
    "tensora_ = channel_shuffle(tensora, groups=2)\n",
    "tensora_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1db5a608-5238-4007-8a7c-9b0f2092b2fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 20, 4, 4])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conva = conv3x3(10, 20)\n",
    "convb = conv1x1(10, 20)\n",
    "tensorb = torch.randn(3,10,4,4)\n",
    "convb(tensorb).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
