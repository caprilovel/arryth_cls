{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03506541-768d-465e-bee9-32f3f261f7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils  import weight_norm  \n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fec4cac7-e1e6-4da9-8ab0-df369b649869",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Chomp1d(nn.Module):\n",
    "    \"\"\"chomp function \n",
    "    \n",
    "    Crop the last few bits of the 1d tensor.\n",
    "    \"\"\"\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9752a7f7-4b35-4946-9f10-893e8cb05753",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1],\n",
       "         [ 3,  4],\n",
       "         [ 6,  7]],\n",
       "\n",
       "        [[ 9, 10],\n",
       "         [12, 13],\n",
       "         [15, 16]],\n",
       "\n",
       "        [[18, 19],\n",
       "         [21, 22],\n",
       "         [24, 25]],\n",
       "\n",
       "        [[27, 28],\n",
       "         [30, 31],\n",
       "         [33, 34]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(36).reshape((4,3,3))\n",
    "chomp = Chomp1d(1)\n",
    "chomp(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b2c3c0c-3dc2-4c08-bca5-2489856133e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3437141/4185335321.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTemporalBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     def __init__(self, n_inputs, n_outputs, kernel_size,\n\u001b[1;32m      3\u001b[0m                  stride, dilation, padding, dropout=0.2):\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTemporalBlock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size,\n",
    "                 stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout2d(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout2d(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2,\n",
    "                                )\n",
    "        \n",
    "        \n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else  nn.Identity()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        #self.conv1.weight.data.normal_(0, 0.01)\n",
    "        nn.init.xavier_uniform(self.conv1.weight, gain=np.sqrt(2))\n",
    "        #self.conv2.weight.data.normal_(0, 0.01)\n",
    "        nn.init.xavier_uniform(self.conv2.weight, gain=np.sqrt(2))\n",
    "        if type(self.downsample) == nn.Identity :\n",
    "            #self.downsample.weight.data.normal_(0, 0.01)\n",
    "            nn.init.xavier_uniform(self.downsample.weight, gain=np.sqrt(2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        net = self.net(x)\n",
    "        res = self.downsample(x)\n",
    "        return self.relu(net + res)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d299738-077c-4f20-8fbb-cc356d8762ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idt = nn.Identity()\n",
    "type(idt) == nn.Identity\n",
    "# idt(torch.arange(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a67adf7d-7892-4150-96fd-c6bd78c58a93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2, max_length=200, attention=False):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                   padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "            if attention == True:\n",
    "                layers += [AttentionBlock(max_length, max_length, max_length)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4baa1c5b-15ee-4e9b-b1c0-a57b41f478c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"An attention mechanism similar to Vaswani et al (2017)\n",
    "    The input of the AttentionBlock is `BxTxD` where `B` is the input\n",
    "    minibatch size, `T` is the length of the sequence `D` is the dimensions of\n",
    "    each feature.\n",
    "    The output of the AttentionBlock is `BxTx(D+V)` where `V` is the size of the\n",
    "    attention values.\n",
    "    Args:\n",
    "      dims (int): the number of dimensions (or channels) of each element in\n",
    "          the input sequence\n",
    "      k_size (int): the size of the attention keys\n",
    "      v_size (int): the size of the attention values\n",
    "      seq_len (int): the length of the input and output sequences\n",
    "    \"\"\"\n",
    "    def __init__(self, dims, k_size, v_size, seq_len=None):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.key_layer = nn.Linear(dims, k_size)\n",
    "        self.query_layer = nn.Linear(dims, k_size)\n",
    "        self.value_layer = nn.Linear(dims, v_size)\n",
    "        self.sqrt_k = math.sqrt(k_size)\n",
    "\n",
    "    def forward(self, minibatch):\n",
    "        keys = self.key_layer(minibatch)\n",
    "        queries = self.query_layer(minibatch)\n",
    "        values = self.value_layer(minibatch)\n",
    "        logits = torch.bmm(queries, keys.transpose(2,1))\n",
    "        # Use numpy triu because you can't do 3D triu with PyTorch\n",
    "        # TODO: using float32 here might break for non FloatTensor inputs.\n",
    "        # Should update this later to use numpy/PyTorch types of the input.\n",
    "        mask = np.triu(np.ones(logits.size()), k=1).astype('uint8')\n",
    "        mask = torch.from_numpy(mask).cuda()\n",
    "        # do masked_fill_ on data rather than Variable because PyTorch doesn't\n",
    "        # support masked_fill_ w/-inf directly on Variables for some reason.\n",
    "        logits.data.masked_fill_(mask, float('-inf'))\n",
    "        probs = F.softmax(logits, dim=1) / self.sqrt_k\n",
    "        read = torch.bmm(probs, values)\n",
    "        return minibatch + read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57575d6-7d1a-47af-8b76-91f791a64337",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
